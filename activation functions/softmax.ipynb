{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fad8847a",
   "metadata": {},
   "source": [
    "## ðŸ§  Softmax Activation Function\n",
    "\n",
    "The **Softmax function** converts a vector of logits (raw scores) into a **probability distribution** across multiple classes, where the sum of probabilities equals 1. It is commonly used in **multi-class classification**.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$$`\n",
    "\n",
    "- \\(z_i\\) : logit for class i  \n",
    "- \\(K\\) : number of classes  \n",
    "\n",
    "**Key Properties:**\n",
    "\n",
    "- Outputs probabilities: all values are positive and sum to 1  \n",
    "- Highlights differences in logits via exponential scaling  \n",
    "- Differentiable â†’ usable in backpropagation  \n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Use Cases\n",
    "\n",
    "- Multi-class classification (e.g., image recognition, NLP tasks)  \n",
    "- Final layer of neural networks to produce probabilities  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŒŸ Advantages\n",
    "\n",
    "- Produces **interpretable probabilities**  \n",
    "- **Normalized output** â†’ sum equals 1  \n",
    "- Differentiable â†’ integrates with gradient-based optimization  \n",
    "\n",
    "---\n",
    "\n",
    "### âš ï¸ Disadvantages\n",
    "\n",
    "- Sensitive to outliers / extreme logits  \n",
    "- Can produce very small gradients â†’ slower training  \n",
    "- Computationally expensive for large numbers of classes  \n",
    "- Not suitable for multi-label classification  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“ˆ Cross-Entropy Loss (with Softmax)\n",
    "\n",
    "Used to train multi-class models:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\sum_{i=1}^{K} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "- \\(y_i\\) : true label (1 for correct class, 0 otherwise)  \n",
    "- \\(\\hat{y}_i\\) : predicted probability from Softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1c9e814",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msoftmax\u001b[39m(x):\n\u001b[32m      3\u001b[39m     exp_x = np.exp(x - np.max(x))  \u001b[38;5;66;03m# for numerical stability\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # for numerical stability\n",
    "    return exp_x / exp_x.sum(axis=0)\n",
    "\n",
    "# Example usage\n",
    "logits = np.array([2.0, 1.0, 0.1])\n",
    "probabilities = softmax(logits)\n",
    "print(\"Softmax Probabilities:\", probabilities)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbc259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Activation Functions\n",
    "# -----------------------------\n",
    "def softmax(logits):\n",
    "    \"\"\"\n",
    "    Softmax function for multi-class classification.\n",
    "    Numerical stability improved by subtracting max logit per sample.\n",
    "    \"\"\"\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"ReLU activation: sets negative values to 0.\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "# -----------------------------\n",
    "# Neural Network Class\n",
    "# -----------------------------\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        Layer 1: Hidden layer with ReLU\n",
    "        Layer 2: Output layer with Softmax\n",
    "        \"\"\"\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.A1 = relu(self.Z1)\n",
    "        \n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = softmax(self.Z2)  # Output probabilities\n",
    "        \n",
    "        return self.A2\n",
    "    \n",
    "    def compute_loss(self, Y, Y_hat):\n",
    "        \"\"\"\n",
    "        Compute cross-entropy loss.\n",
    "        Y: true labels (class indices)\n",
    "        Y_hat: predicted probabilities from Softmax\n",
    "        \"\"\"\n",
    "        m = Y.shape[0]\n",
    "        log_likelihood = -np.log(Y_hat[range(m), Y])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss\n",
    "\n",
    "# -----------------------------\n",
    "# Example Usage\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize network: 3 input features, 5 hidden neurons, 3 output classes\n",
    "    nn = NeuralNetwork(input_size=3, hidden_size=5, output_size=3)\n",
    "    \n",
    "    # Input data (5 samples, 3 features each)\n",
    "    X = np.array([\n",
    "        [1.0, 2.0, 3.0],\n",
    "        [0.5, 1.5, 2.5],\n",
    "        [1.5, 2.5, 3.5],\n",
    "        [2.0, 1.0, 0.5],\n",
    "        [3.0, 3.0, 3.0]\n",
    "    ])\n",
    "    \n",
    "    # True labels\n",
    "    Y = np.array([0, 1, 2, 1, 0])\n",
    "    \n",
    "    # Forward pass\n",
    "    Y_hat = nn.forward(X)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = nn.compute_loss(Y, Y_hat)\n",
    "    \n",
    "    print(\"Softmax Output (Probabilities):\\n\", Y_hat)\n",
    "    print(\"Cross-Entropy Loss:\", loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

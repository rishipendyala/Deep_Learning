{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0051c9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250fb770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, rnn_type='LSTM',\n",
    "                 num_layers=1, dropout=0.0, batch_norm=False, bidirectional=False):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn_type = rnn_type.upper()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_norm = batch_norm\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        # RNN Layer\n",
    "        if self.rnn_type == 'RNN':\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, num_layers=num_layers,\n",
    "                              batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "                              bidirectional=bidirectional)\n",
    "        elif self.rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
    "                               batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "                               bidirectional=bidirectional)\n",
    "        elif self.rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, num_layers=num_layers,\n",
    "                              batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "                              bidirectional=bidirectional)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported RNN type\")\n",
    "\n",
    "        # BatchNorm\n",
    "        if batch_norm:\n",
    "            self.bn = nn.BatchNorm1d(hidden_size * self.num_directions)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size * self.num_directions, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        out = rnn_out[:, -1, :]  # last timestep output\n",
    "\n",
    "        if self.batch_norm:\n",
    "            out = self.bn(out)\n",
    "        out = self.dropout_layer(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630fd508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(model, optimizer_name='adam', lr=0.001, reg_lambda=0.0):\n",
    "    if optimizer_name.lower() == 'adam':\n",
    "        return optim.Adam(model.parameters(), lr=lr, weight_decay=reg_lambda)\n",
    "    elif optimizer_name.lower() == 'sgd':\n",
    "        return optim.SGD(model.parameters(), lr=lr, weight_decay=reg_lambda)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "def get_loss_fn(loss_name='mse'):\n",
    "    if loss_name.lower() == 'mse':\n",
    "        return nn.MSELoss()\n",
    "    elif loss_name.lower() == 'crossentropy':\n",
    "        return nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported loss function\")\n",
    "\n",
    "# -------------------------\n",
    "# Training Loop\n",
    "# -------------------------\n",
    "def train_rnn(model, optimizer, loss_fn, train_loader, val_loader=None, \n",
    "              epochs=50, device='cpu', early_stopping=False, patience=5, \n",
    "              use_scheduler=False, task_type='regression', sequence_mask=None):\n",
    "    \"\"\"\n",
    "    task_type: 'regression', 'classification_last', 'classification_sequence'\n",
    "    sequence_mask: tensor of shape (batch, seq_len) with 1 for valid timesteps, 0 for padding\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    best_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    scheduler = None\n",
    "    if use_scheduler:\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            # Handle sequence classification masking\n",
    "            if task_type == 'classification_sequence' and sequence_mask is not None:\n",
    "                mask = sequence_mask.to(device)\n",
    "                y_pred = y_pred.view(-1, y_pred.size(-1))\n",
    "                y_batch = y_batch.view(-1)\n",
    "                mask = mask.view(-1)\n",
    "                y_pred = y_pred[mask == 1]\n",
    "                y_batch = y_batch[mask == 1]\n",
    "\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for X_val, y_val in val_loader:\n",
    "                    X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                    y_val_pred = model(X_val)\n",
    "\n",
    "                    if task_type == 'classification_sequence' and sequence_mask is not None:\n",
    "                        mask = sequence_mask.to(device)\n",
    "                        y_val_pred = y_val_pred.view(-1, y_val_pred.size(-1))\n",
    "                        y_val = y_val.view(-1)\n",
    "                        mask = mask.view(-1)\n",
    "                        y_val_pred = y_val_pred[mask == 1]\n",
    "                        y_val = y_val[mask == 1]\n",
    "\n",
    "                    loss_val = loss_fn(y_val_pred, y_val)\n",
    "                    val_loss += loss_val.item() * X_val.size(0)\n",
    "            val_loss /= len(val_loader.dataset)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if early_stopping:\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    counter = 0\n",
    "                else:\n",
    "                    counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(\"Early stopping triggered!\")\n",
    "                    break\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        if use_scheduler:\n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40db4a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic dataset\n",
    "seq_len = 10\n",
    "input_size = 5\n",
    "hidden_size = 16\n",
    "output_size = 3  # For classification\n",
    "batch_size = 8\n",
    "n_samples = 100\n",
    "\n",
    "torch.manual_seed(64)\n",
    "\n",
    "X = torch.randn(n_samples, seq_len, input_size)\n",
    "y = torch.randint(0, output_size, (n_samples,)) \n",
    "\n",
    "# create train and validation loaders\n",
    "dataset = TensorDataset(X, y)\n",
    "train_size = int(0.8 * n_samples)\n",
    "val_size = n_samples - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "# Model, optimizer, loss function\n",
    "model = RNN(input_size, hidden_size, output_size, rnn_type='LSTM'\n",
    "            ,num_layers=2, dropout=0.2, batch_norm=True, bidirectional=True)\n",
    "optimizer = build_optimizer(model, optimizer_name='adam', lr=0.001, reg_lambda\n",
    "=0.0001)\n",
    "loss_fn = get_loss_fn(loss_name='crossentropy')\n",
    "# Train the model\n",
    "train_rnn(model, optimizer, loss_fn, train_loader, val_loader,\n",
    "            epochs=100, device='cpu', early_stopping=True, patience=23, \n",
    "            use_scheduler=True, task_type='classification_last')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b563b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class OneToOneRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(OneToOneRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = out[:, -1, :]  # last timestep\n",
    "        out = self.fc(out)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "# Example usage\n",
    "model = OneToOneRNN(input_size=10, hidden_size=32)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c53310",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneToManyRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_seq_len, vocab_size):\n",
    "        super(OneToManyRNN, self).__init__()\n",
    "        self.fc_in = nn.Linear(input_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.output_seq_len = output_seq_len\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x).unsqueeze(1)  # add time dimension\n",
    "        x = x.repeat(1, self.output_seq_len, 1)  # repeat across time steps\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.fc_out(out)\n",
    "\n",
    "# Example usage\n",
    "model = OneToManyRNN(input_size=2048, hidden_size=128, output_seq_len=20, vocab_size=10000)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3383df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManyToOneRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, rnn_type='LSTM'):\n",
    "        super(ManyToOneRNN, self).__init__()\n",
    "        if rnn_type.upper() == 'LSTM':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        elif rnn_type.upper() == 'GRU':\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, 32)\n",
    "        self.fc2 = nn.Linear(32, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = out[:, -1, :]  # last timestep\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        return self.fc2(out)\n",
    "\n",
    "# Example usage\n",
    "model = ManyToOneRNN(input_size=128, hidden_size=64, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e37ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, hidden = self.rnn(x)\n",
    "        return hidden  # pass context to decoder\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, seq_len):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.rnn = nn.RNN(output_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        x = x.repeat(1, self.seq_len, 1)  # initial decoder input\n",
    "        out, _ = self.rnn(x, hidden)\n",
    "        return self.fc(out)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, output_seq_len):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(input_size, hidden_size)\n",
    "        self.decoder = Decoder(output_size, hidden_size, output_seq_len)\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        hidden = self.encoder(src)\n",
    "        return self.decoder(tgt, hidden)\n",
    "\n",
    "# Example usage\n",
    "input_seq_len, output_seq_len = 50, 60\n",
    "model = Seq2Seq(input_size=10000, hidden_size=256, output_size=12000, output_seq_len=output_seq_len)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b0c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# Example BiRNN Model\n",
    "# ----------------------------\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, output_size=1, num_layers=1, dropout=0.5):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_size, num_layers=num_layers,\n",
    "                          batch_first=True, bidirectional=True, nonlinearity='tanh')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)  # bidirectional → hidden*2\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)               # [batch, seq_len] -> [batch, seq_len, embed_dim]\n",
    "        out, _ = self.rnn(x)                # [batch, seq_len, hidden*2]\n",
    "        out = out[:, -1, :]                 # take last timestep\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# ----------------------------\n",
    "# Hyperparameters\n",
    "# ----------------------------\n",
    "vocab_size = 2000\n",
    "embed_dim = 128\n",
    "hidden_size = 64\n",
    "max_len = 50\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "lr = 0.001\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ----------------------------\n",
    "# Simulate data (replace with actual IMDb preprocessing)\n",
    "# ----------------------------\n",
    "X_train = np.random.randint(0, vocab_size, (1000, max_len))\n",
    "y_train = np.random.randint(0, 2, (1000, 1))\n",
    "\n",
    "X_test = np.random.randint(0, vocab_size, (200, max_len))\n",
    "y_test = np.random.randint(0, 2, (200, 1))\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_dataset = TensorDataset(torch.LongTensor(X_train), torch.FloatTensor(y_train))\n",
    "test_dataset = TensorDataset(torch.LongTensor(X_test), torch.FloatTensor(y_test))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# ----------------------------\n",
    "# Model, Loss, Optimizer\n",
    "# ----------------------------\n",
    "model = BiRNN(vocab_size, embed_dim, hidden_size).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    y_true, y_pred_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            y_true.extend(y_batch.cpu().numpy())\n",
    "            y_pred_all.extend((y_pred.cpu().numpy() > 0.5).astype(int))\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred_all)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Test Acc: {acc:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Classification Report\n",
    "# ----------------------------\n",
    "print(classification_report(y_true, y_pred_all, target_names=['Negative', 'Positive']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff26b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bidirectional LSTM (BiLSTM) for Sentiment Analysis in PyTorch\n",
    "\n",
    "Theory:\n",
    "---------\n",
    "1. BiLSTM is an extension of standard LSTM that processes sequences in both forward\n",
    "   and backward directions, capturing past and future context simultaneously.\n",
    "   \n",
    "2. Each BiLSTM layer contains two LSTMs:\n",
    "   - Forward LSTM: processes the sequence from start to end.\n",
    "   - Backward LSTM: processes the sequence from end to start.\n",
    "   \n",
    "3. Outputs from both directions are concatenated to form the final representation:\n",
    "   final_output_t = forward_output_t + backward_output_t\n",
    "\n",
    "4. This makes BiLSTMs effective for NLP tasks like sentiment analysis, where context\n",
    "   from both past and future words can improve classification performance.\n",
    "\n",
    "5. Typical architecture:\n",
    "   - Text tokenization/vectorization\n",
    "   - Embedding layer\n",
    "   - One or more BiLSTM layers (with optional dropout)\n",
    "   - Dense layers for classification\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset Preparation\n",
    "# ----------------------------\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "max_len = 100\n",
    "batch_size = 32\n",
    "\n",
    "# Custom dataset class\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, split='train'):\n",
    "        self.data = list(IMDB(split=split))\n",
    "        self.texts = [text for (label, text) in self.data]\n",
    "        self.labels = [0 if label=='neg' else 1 for (label, text) in self.data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = tokenizer(self.texts[idx])\n",
    "        # Truncate/pad sequence\n",
    "        tokens = tokens[:max_len] + ['<pad>']*(max_len - len(tokens)) if len(tokens) < max_len else tokens[:max_len]\n",
    "        return tokens, self.labels[idx]\n",
    "\n",
    "# Build vocabulary\n",
    "def yield_tokens(dataset):\n",
    "    for tokens, _ in dataset:\n",
    "        yield tokens\n",
    "\n",
    "train_dataset = IMDBDataset('train')\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_dataset), specials=[\"<unk>\", \"<pad>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Collate function\n",
    "def collate_batch(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    text_ids = torch.tensor([vocab(t) for t in texts], dtype=torch.long)\n",
    "    labels = torch.tensor(labels, dtype=torch.float).unsqueeze(1)\n",
    "    return text_ids, labels\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(IMDBDataset('test'), batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "# ----------------------------\n",
    "# BiLSTM Model\n",
    "# ----------------------------\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, hidden_size1=64, hidden_size2=32, output_size=1, dropout=0.4):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab[\"<pad>\"])\n",
    "        self.bilstm1 = nn.LSTM(embed_dim, hidden_size1, batch_first=True, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.bilstm2 = nn.LSTM(hidden_size1*2, hidden_size2, batch_first=True, bidirectional=True)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size2*2, 64)\n",
    "        self.fc2 = nn.Linear(64, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)                # [batch, seq_len, embed_dim]\n",
    "        x, _ = self.bilstm1(x)              # [batch, seq_len, hidden1*2]\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.bilstm2(x)              # [batch, seq_len, hidden2*2]\n",
    "        x = self.dropout2(x)\n",
    "        x = x[:, -1, :]                      # take last timestep\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# ----------------------------\n",
    "# Training Setup\n",
    "# ----------------------------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = BiLSTMModel(len(vocab)).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 3\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    avg_loss = sum(train_losses)/len(train_losses)\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    y_true, y_pred_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            y_true.extend(y_batch.cpu().numpy())\n",
    "            y_pred_all.extend((y_pred.cpu().numpy()>0.5).astype(int))\n",
    "    acc = accuracy_score(y_true, y_pred_all)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_loss:.4f} | Test Acc: {acc:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Classification Report\n",
    "# ----------------------------\n",
    "print(classification_report(y_true, y_pred_all, target_names=['Negative', 'Positive']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9db760",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Many-to-Many Bidirectional LSTM (BiLSTM) for Sequence Labeling (NER/POS Tagging)\n",
    "\n",
    "Theory:\n",
    "--------\n",
    "1. Many-to-Many RNNs take an input sequence and output a sequence of the same length,\n",
    "   making them suitable for tasks like Named Entity Recognition (NER) or POS tagging.\n",
    "\n",
    "2. Bidirectional LSTMs capture both past and future context for each token by processing\n",
    "   sequences in forward and backward directions. The hidden states are concatenated.\n",
    "\n",
    "3. Architecture:\n",
    "   - Embedding layer: converts tokens to dense vectors.\n",
    "   - BiLSTM layer(s): output hidden states for each timestep.\n",
    "   - Linear layer: maps hidden states at each timestep to class scores (num_classes).\n",
    "   - Softmax (or CrossEntropyLoss): used for predicting token-level labels.\n",
    "\n",
    "4. Output:\n",
    "   For input sequence of shape [batch, seq_len], the output is [batch, seq_len, num_classes].\n",
    "   Each timestep is classified independently using context-aware BiLSTM hidden states.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ----------------------------\n",
    "# Example Dataset (simulated)\n",
    "# ----------------------------\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, num_samples=500, seq_len=10, vocab_size=50, num_classes=5):\n",
    "        self.X = np.random.randint(1, vocab_size, size=(num_samples, seq_len))\n",
    "        self.y = np.random.randint(0, num_classes, size=(num_samples, seq_len))\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = SequenceDataset()\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Many-to-Many BiLSTM Model\n",
    "# ----------------------------\n",
    "class ManyToManyBiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, hidden_size=128, num_classes=5, dropout=0.3):\n",
    "        super(ManyToManyBiLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.bilstm = nn.LSTM(embed_dim, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)  # hidden_size*2 because of bidirectional\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)               # [batch, seq_len, embed_dim]\n",
    "        x, _ = self.bilstm(x)              # [batch, seq_len, hidden_size*2]\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)                      # [batch, seq_len, num_classes]\n",
    "        return x\n",
    "\n",
    "# ----------------------------\n",
    "# Training Setup\n",
    "# ----------------------------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = ManyToManyBiLSTM(vocab_size=50, num_classes=5).to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # expects input [batch*seq_len, num_classes] and target [batch*seq_len]\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 5\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)  # [batch, seq_len, num_classes]\n",
    "        \n",
    "        # Reshape for CrossEntropyLoss\n",
    "        y_pred = y_pred.view(-1, y_pred.shape[2])  # [batch*seq_len, num_classes]\n",
    "        y_batch = y_batch.view(-1)                 # [batch*seq_len]\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluation (on training data for example)\n",
    "# ----------------------------\n",
    "model.eval()\n",
    "y_true_all, y_pred_all = [], []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        y_pred = model(X_batch)\n",
    "        y_pred_labels = torch.argmax(y_pred, dim=2)\n",
    "        y_true_all.extend(y_batch.cpu().numpy().flatten())\n",
    "        y_pred_all.extend(y_pred_labels.cpu().numpy().flatten())\n",
    "\n",
    "print(classification_report(y_true_all, y_pred_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed93a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gated Recurrent Unit (GRU) Networks\n",
    "\n",
    "GRUs are a type of Recurrent Neural Network (RNN) introduced to efficiently model sequential data such as time-series, text, and speech.\n",
    "While traditional RNNs struggle with long-term dependencies due to vanishing gradients, GRUs use gating mechanisms to selectively \n",
    "update the hidden state at each time step.\n",
    "\n",
    "GRU Components:\n",
    "1. Update Gate (z_t): Controls how much of the previous hidden state is retained for the next step.\n",
    "2. Reset Gate (r_t): Determines how much of the past hidden state should be forgotten.\n",
    "3. Candidate hidden state (h_t'): Computed using the reset gate and current input.\n",
    "4. Hidden state (h_t): Weighted average of previous hidden state h_{t-1} and candidate h_t', controlled by the update gate.\n",
    "\n",
    "GRU Equations:\n",
    "r_t = σ(W_r * [h_{t-1}, x_t])\n",
    "z_t = σ(W_z * [h_{t-1}, x_t])\n",
    "h_t' = tanh(W_h * [r_t * h_{t-1}, x_t])\n",
    "h_t = (1 - z_t) * h_{t-1} + z_t * h_t'\n",
    "\n",
    "Advantages of GRUs:\n",
    "- Simplified architecture compared to LSTM (2 gates vs 3)\n",
    "- Faster training due to fewer parameters\n",
    "- Performs similarly to LSTM in many sequential tasks\n",
    "\n",
    "Typical Usage:\n",
    "- Time-series forecasting\n",
    "- NLP tasks\n",
    "- Speech recognition\n",
    "- Sequence classification\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load Dataset\n",
    "# ---------------------------\n",
    "df = pd.read_csv('data.csv', parse_dates=['Date'], index_col='Date')\n",
    "data = df.values\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Preprocess Data\n",
    "# ---------------------------\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step - 1):\n",
    "        X.append(data[i:(i + time_step), 0])\n",
    "        y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_step = 100\n",
    "X, y = create_dataset(scaled_data, time_step)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.from_numpy(X).float().unsqueeze(-1)  # shape: [samples, time_step, features]\n",
    "y = torch.from_numpy(y).float().unsqueeze(-1)  # shape: [samples, 1]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Define GRU Model\n",
    "# ---------------------------\n",
    "class GRUTimeSeries(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=50, num_layers=2, output_size=1, dropout=0.2):\n",
    "        super(GRUTimeSeries, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # GRU layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers=num_layers, \n",
    "                          batch_first=True, dropout=dropout)\n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # GRU forward pass\n",
    "        out, _ = self.gru(x, h0)  # out: [batch, seq_len, hidden_size]\n",
    "\n",
    "        # Take the last timestep output for prediction\n",
    "        out = self.fc(out[:, -1, :])  # shape: [batch, output_size]\n",
    "        return out\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Train the Model\n",
    "# ---------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GRUTimeSeries().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "X_train, y_train = X.to(device), y.to(device)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    permutation = torch.randperm(X_train.size(0))\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i in range(0, X_train.size(0), batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = X_train[indices], y_train[indices]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "    epoch_loss /= X_train.size(0)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Make Predictions\n",
    "# ---------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    input_seq = X_train[-1].unsqueeze(0)  # take last sequence\n",
    "    predicted = model(input_seq)\n",
    "    predicted_value = scaler.inverse_transform(predicted.cpu().numpy())\n",
    "    print(f\"Predicted value for next step: {predicted_value[0][0]:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

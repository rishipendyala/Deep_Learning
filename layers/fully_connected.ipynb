{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "489209e9",
   "metadata": {},
   "source": [
    "# ðŸ§  Fully Connected (Dense) Layers in Deep Learning\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "A **Fully Connected (FC)** layer â€” also called a **Dense layer** â€” is a type of neural network layer where **each neuron in one layer connects to every neuron in the next layer**.\n",
    "\n",
    "- In **CNNs**, FC layers appear after convolutional and pooling layers to interpret feature maps and produce final predictions.  \n",
    "- In **feedforward networks**, FC layers are the **core computational layers** that transform raw inputs into learned representations and outputs.\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Structure\n",
    "\n",
    "Each neuron in an FC layer:\n",
    "- Receives input from **all neurons in the previous layer**  \n",
    "- Sends output to **all neurons in the next layer**\n",
    "\n",
    "This creates **dense interconnections**, enabling global information integration across the network.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”© Key Components\n",
    "\n",
    "| Component | Description |\n",
    "|------------|--------------|\n",
    "| **Neurons** | Compute activations by applying weights, bias, and activation function |\n",
    "| **Weights** $(w_{ij})$ | Control the strength of connection between neurons $i \\rightarrow j$ |\n",
    "| **Bias** $(b_j)$ | Shifts the output independent of the input |\n",
    "| **Activation Function** $f(\\cdot)$ | Introduces non-linearity (e.g., ReLU, Sigmoid, Tanh) |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Working of a Fully Connected Layer\n",
    "\n",
    "### 1. Linear Transformation\n",
    "\n",
    "Each neuron computes a **weighted sum** of all inputs plus a bias:\n",
    "\n",
    "$$\n",
    "z_j = \\sum_i (w_{ij} \\cdot x_i) + b_j\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- \\( w_{ij} \\) â†’ weight from neuron *i* to *j*  \n",
    "- \\( x_i \\) â†’ input from neuron *i*  \n",
    "- \\( b_j \\) â†’ bias for neuron *j*\n",
    "\n",
    "### 2. Non-Linear Activation\n",
    "\n",
    "The output is then passed through an **activation function** to introduce non-linearity:\n",
    "\n",
    "$$\n",
    "a_j = f(z_j)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© Example Configuration\n",
    "\n",
    "**Example:**  \n",
    "A network transitions from **4 neurons â†’ 3 neurons (FC layer)**  \n",
    "\n",
    "- Total **weights** = \\( 4 \\times 3 = 12 \\)  \n",
    "- Total **biases** = 3  \n",
    "- Each neuron in the FC layer is connected to all 4 previous neurons.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Roles of Fully Connected Layers\n",
    "\n",
    "1. **Feature Integration & Abstraction**  \n",
    "   Combines features extracted by earlier layers (e.g., convolutional layers) into unified representations.\n",
    "\n",
    "2. **Decision Making & Output Generation**  \n",
    "   Converts high-level features into class scores or continuous outputs (often followed by Softmax or Sigmoid).\n",
    "\n",
    "3. **Non-Linearity Introduction**  \n",
    "   Activation functions like ReLU, Sigmoid, or Tanh help learn complex, non-linear relationships.\n",
    "\n",
    "4. **Universal Approximation**  \n",
    "   By the **Universal Approximation Theorem**, an FC layer with sufficient neurons can approximate **any continuous function**.\n",
    "\n",
    "5. **Flexibility Across Domains**  \n",
    "   Used in **vision**, **speech**, and **NLP** models â€” FC layers are **input-agnostic** and versatile.\n",
    "\n",
    "6. **Regularization & Overfitting Control**  \n",
    "   Techniques like **Dropout**, **BatchNorm**, or **L2 regularization** help improve generalization.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Advantages\n",
    "\n",
    "- **Feature Integration:** Combines all learned representations for final decision-making  \n",
    "- **Flexibility:** Works across architectures and data types (tabular, text, images)  \n",
    "- **Simplicity:** Easy to implement and supported by all deep learning frameworks  \n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ Limitations\n",
    "\n",
    "| Limitation | Description |\n",
    "|-------------|--------------|\n",
    "| **High Computational Cost** | Dense connectivity â†’ large number of parameters â†’ high memory and compute load |\n",
    "| **Prone to Overfitting** | Many parameters â†’ easily overfits on small datasets without regularization |\n",
    "| **Ignores Spatial Structure** | Fails to exploit local patterns in data (e.g., pixels in an image) unlike CNN layers |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§® Summary\n",
    "\n",
    "Fully Connected layers serve as the **final interpreters** in deep networks â€” they aggregate, transform, and map learned features into predictions.  \n",
    "They are **powerful but computationally expensive**, and work best when combined with regularization.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© PyTorch Example\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example: Simple Fully Connected Network\n",
    "class FCNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)   # Input -> Hidden\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)    # Hidden -> Output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = FCNetwork()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdd9061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Basic structure\n",
    "input_neurons = 4\n",
    "output_neurons = 3\n",
    "\n",
    "# Coordinates for neurons\n",
    "x_input = np.zeros(input_neurons)\n",
    "y_input = np.linspace(-1, 1, input_neurons)\n",
    "\n",
    "x_output = np.ones(output_neurons) * 2\n",
    "y_output = np.linspace(-0.8, 0.8, output_neurons)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# Draw input neurons\n",
    "for i in range(input_neurons):\n",
    "    ax.scatter(x_input[i], y_input[i], s=1000, color=\"#AEDFF7\", edgecolors=\"k\", zorder=3)\n",
    "    ax.text(x_input[i]-0.3, y_input[i], f\"$x_{i+1}$\", fontsize=13, va=\"center\", ha=\"center\")\n",
    "\n",
    "# Draw output neurons\n",
    "for j in range(output_neurons):\n",
    "    ax.scatter(x_output[j], y_output[j], s=1000, color=\"#A3E4A7\", edgecolors=\"k\", zorder=3)\n",
    "    ax.text(x_output[j]+0.3, y_output[j], f\"$z_{j+1}$\", fontsize=13, va=\"center\", ha=\"center\")\n",
    "\n",
    "# Draw connections (weights)\n",
    "for i in range(input_neurons):\n",
    "    for j in range(output_neurons):\n",
    "        ax.plot([x_input[i]+0.1, x_output[j]-0.1], [y_input[i], y_output[j]], \n",
    "                 color=\"gray\", linewidth=1.2, alpha=0.8)\n",
    "        # Annotate one or two sample weights for demonstration\n",
    "        if (i == 0 and j == 0) or (i == 3 and j == 2):\n",
    "            mid_x = (x_input[i] + x_output[j]) / 2\n",
    "            mid_y = (y_input[i] + y_output[j]) / 2\n",
    "            ax.text(mid_x, mid_y + 0.05, f\"$w_{{{i+1},{j+1}}}$\", fontsize=11, color=\"black\")\n",
    "\n",
    "# Add bias term representation\n",
    "ax.text(1, 1.1, r\"$b_j$\", fontsize=14, color=\"black\")\n",
    "ax.arrow(1, 1.0, 0.6, -0.1, head_width=0.05, head_length=0.1, fc=\"black\", ec=\"black\")\n",
    "\n",
    "# Title and annotation\n",
    "ax.text(1, -1.25, \"Fully Connected (Dense) Layer: $z_j = \\\\sum_i w_{ij}x_i + b_j$\", \n",
    "        fontsize=14, ha=\"center\", va=\"center\", style=\"italic\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
